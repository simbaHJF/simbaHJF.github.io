---
layout:     post
title:      "零拷贝"
date:       2019-10-12 14:30:00 +0800
author:     "simba"
header-img: "img/post-bg-miui6.jpg"
tags:
    - linux

---

> 转:石杉的架构笔记


##	零拷贝(Zero-copy)简介

零拷贝(Zero-copy)技术指在计算机执行操作时,CPU不需要先将数据从一个内存区域复制到另一个内存区域,从而可以减少上下文切换以及CPU的拷贝时间.

它的作用是在数据报从网络设备到用户程序空间传递的过程中,减少数据拷贝次数,减少系统调用,实现CPU的零参与,彻底消除CPU在这方面的负载.

实现零拷贝用到的最主要技术是DMA数据传输技术和内存区域映射技术:
*	零拷贝机制可以减少数据在内核缓冲区和用户进程缓冲区之间反复的I/O拷贝操作.
*	零拷贝机制可以减少用户进程地址空间和内核地址空间之间因为上下文切换而带来的CPU开销.


##	物理内存和虚拟内存

####	物理内存

物理内存(Physical Memory)是相对于虚拟内存(Virtual Memory)而言的.物理内存指通过物理内存条而获得的内存空间;而虚拟内存则是划分的一块虚拟的区域.内存主要作用是在计算机运行时为操作系统和各种程序提供临时储存.

####	虚拟内存

虚拟内存是计算机系统内存管理的一种技术.它使得应用程序认为它拥有连续的可用的内存(一个连续完整的地址空间).而实际上,虚拟内存通常是被分割成多个物理内存碎片,还有部分暂时存储在外部磁盘存储器上,在需要时进行数据交换,加载到物理内存中来.

虚拟内存地址和用户进程紧密相关,一般来说不同进程里的同一个虚拟地址指向的物理地址是不一样的,所以离开进程谈虚拟内存没有任何意义.每个进程所能使用的虚拟地址大小和CPU位数有关.虚拟地址空间又分为内核空间和用户空间两部分.32位系统的内核空间占用1G,位于高位,剩下的3G是用户空间.而64位系统的内核空间和用户空间都是128T,分别占据整个内存空间的最高处和最低处,剩下的中间部分是未定义的.而实际的物理内存可能远远小于虚拟内存的大小.

每个用户进程维护了一个单独的页表(Page Table),虚拟内存和物理内存就是通过这个页表实现地址空间的映射的.

下面给出两个进程A,B各自的虚拟内存空间以及对应的物理内存之间的地址映射示意图:
![uOhyZD.png](https://s2.ax1x.com/2019/10/12/uOhyZD.png)

当进程执行一个程序时,需要先从内存中读取该进程的指令,然后执行,获取指令时用到的就是虚拟地址.这个虚拟地址是程序链接时确定的(内核加载并初始化进程时会调整动态库的地址范围).

为了获取到实际的数据,CPU需要将虚拟地址转换成物理地址,CPU转换地址时需要用到进程的页表(Page Table),而页表(Page Table)里面的数据由操作系统维护.页表(Page Table)维护着内存映射(Memory Mapping),里面的每个内存映射都将一块虚拟地址映射到一个特定的地址空间(物理内存或者磁盘存储空间).每个进程都拥有自己的页表(Page Table),和其他进程的页表(Page Table)没有关系.

<font color="red">通过上面的介绍,可以简单的将用户进程申请并访问物理内存(或磁盘存储空间)的过程总结如下:</font>
*	用户进程向操作系统发出内存申请请求.
*	系统会检查进程的虚拟地址空间是否被用完,如果有剩余,给进程分配虚拟地址.
*	系统为这块虚拟地址创建内存映射(Memory Mapping),并将它放进该进程的页表(Page Table)
*	系统返回虚拟地址给用户进程,用户进程开始访问该虚拟地址.
*	CPU根据虚拟地址在此进程的页表(Page Table)中找到了相应的内存映射(Memory Mapping),但是这个内存映射没有和物理内存关联,于是产生了缺页中断.
*	操作系统收到缺页中断后,分配真正的物理内存并将它关联到页表相应的内存映射.中断处理完成后,CPU就可以访问内存了.
*	当然缺页中断不是每次都会发生,只有系统觉得有必要延迟分配内存的时候才用得着,也即很多时候,在上面的第3步系统就会分配真正的物理内存并和内存映射进行关联.

<font color="red">在用户进程和物理内存(磁盘存储器)之间引入虚拟内存主要有以下的有点:</font>
*	地址空间:	提供更大的地址空间,并且地址空间是连续的,使得程序编写,链接更加简单.
*	进程隔离:	不同进程的虚拟地址之间没有关系,所以一个进程的操作不会对其他进程造成影响.
*	数据保护:	每块虚拟内存都有相应的读写属性,这样就能保护程序的代码段不被修改,数据块不能被执行等,增加了系统的安全性.
*	内存映射:	有了虚拟内存之后,可以直接映射磁盘上的文件(可执行文件或动态库)到虚拟地址空间.

	这样可以做到物理内存延时分配,只有在需要读相应的文件的时候,才将它真正的从磁盘上加载到内存中来,而在内存吃紧的时候又可以将这部分内存清空掉,提高物理内存利用效率,并且所有这些对应用程序都是透明的.
*	共享内存:	比如动态库只需要在内存中存储一份,然后将它映射到不同进程的虚拟地址空间中,让进程觉得自己独占了这个文件.

	进程间的内存共享也可以通过映射同一块物理内存到进程的不同虚拟地址空间来实现共享.
*	物理内存管理:	物理地址空间全部由操作系统管理,进程无法直接分配和回收,从而系统可以更好的利用内存,平衡进程间对内存的需求.


##	内核空间和用户空间
操作系统的核心是内核,独立于普通的应用程序,可以访问受保护的内存空间,也有访问底层硬件设备的权限.为了避免用户进程直接操作内核,保证内核安全,操作系统将虚拟内存划分为两部分,一部分是内核空间(Kernel-space),一部分是用户空间(User-space).

在Linux系统中,内核模块运行在内核空间,对应的进程处于内核态;而用户程序运行在用户空间,对应的进程处于用户态.Linux32位系统的寻址空间(虚拟存储空间)为4G(2^32),将最高的1G的字节(从虚拟地址0xC0000000到0xFFFFFFFF)供内核进程使用,称为内核空间;较低的3G字节(从虚拟地址0x00000000到0xBFFFFFFF),供各个用户进程使用,称为用户空间.

下图是一个进程的用户空间和内核空间的内存布局:
![uzfGl9.png](https://s2.ax1x.com/2019/10/14/uzfGl9.png)

####	内核空间
内核空间总是驻留在内存中,它是为操作系统和的内核保留的.应用程序是不允许直接在该区域进行读写或直接调用内核代码定义的函数的.

上图左侧区域为内核进程对应的虚拟内存,按访问权限可以分为进程私有和进程共享两块区域:
*	进程私有的虚拟内存:每个进程都有单独的内核栈,页表,task结构以及mem_map结构等.
*	进程共享的虚拟内存:属于所有进程共享的内存区域,包括物理存储器,内核数据和内核代码区域.

####	用户空间
每个普通的用户进程都有一个单独的用户空间,处于用户态的进程不能访问内核空间中的数据,也不能直接调用内核函数,因此要进行系统调用的时候,就要将进程切换到内核态才行.用户空间包括以下几个内存区域:
*	运行时栈:有编译器自动释放,存放函数的参数值,局部变量和方法返回值等.每当一个函数被调用时,该函数的返回类型和一些调用的信息被存储到栈顶,调用结束后调用信息会被弹出并释放掉内存.
栈区是从高地址位向低地址位增长的,是一块连续的内存区域,最大容量是由系统预先定义好的,申请的栈空间超过这个界限时会提示溢出.
*	运行时堆:用于存放进程运行中被动态分配的内存段,位于BSS和栈中间的地址位.由开发人员申请分配(malloc)和释放(free).堆是从低地址位向高低职位增长,采用链式存储结构.
频繁地malloc/free造成内存空间的不连续,产生大量碎片.当申请堆空间时,库函数按照一定的算法搜索可用的足够大的空间.因此堆的效率比栈要低得多.
*	代码段:存放CPU可以执行的机器指令,该部分内存只能读不能写.通常代码区是共享的,即其他执行程序可以调用它.
*	未初始化的数据段:存放未初始化的全局变量,BSS的数据在程序开始执行之前被初始化为0或NULL.
*	已初始化的数据段:存放已初始化的全局变量,包括静态全局变量,静态局部变量以及常量.
*	内存映射区域:例如将动态库,共享内存等虚拟空间的内存映射到物理空间的内存,一般是mmap函数所分配的虚拟内存空间.


##	Linux的内部层级结构

内核态可以执行任意命令,调用系统的一切资源,而用户态只能执行简单的运算,不能直接调用系统资源.用户态必须通过系统接口(System Call),才能向内核发出指令.
![K9lPqx.png](https://s2.ax1x.com/2019/10/15/K9lPqx.png)

*	内核空间可以访问所有的CPU指令和所有的内存空间,I/O空间和硬件设备
*	用户空间只能访问受限的资源,如果需要特殊权限,可以通过系统调用获取相应的资源.
*	内核空间和用户空间是针对线性地址空间的
*	所有内核进程(线程)公用一个地址空间,而用户进程都有各自的地址空间

有了用户空间和内核空间的划分后,Linux内部层级结构可以分为三部分,从最底层到最上层依次是硬件,内核空间和用户空间,如下图所示:
[![K91Mp4.png](https://s2.ax1x.com/2019/10/15/K91Mp4.png)](https://imgchr.com/i/K91Mp4)

##	Linux I/O读写方式

Linux提供了轮询,I/O中断以及DMA传输这3种磁盘与主存之间的数据传输机制.其中轮询方式是基于死循环对I/O端口进行不断检测.

I/O中断方式是指当数据到达时,磁盘主动向CPU发起中断申请,由CPU自身负责数据的传输过程.

DMA传输则在I/O中断的基础上引入了DMA磁盘控制器,由DMA磁盘控制器负责数据的传输,降低了I/O中断操作对CPU资源的大量消耗.

####	I/O中断原理

在DMA技术出现之前,应用程序与磁盘之间的I/O操作都是通过CPU的中断完成的.
![K93crR.png](https://s2.ax1x.com/2019/10/15/K93crR.png)
每次用户进程读取磁盘数据时,都需要CPU中断,然后发起I/O请求等待数据读取和拷贝完成,每次的I/O
中断都导致CPU的上下文切换:

*	用户进程向CPU发起read系统调用读取数据,由用户态切换为内核态,然后一直阻塞等待数据的返回
*	CPU在接收到指令以后对磁盘发起I/O请求,将磁盘数据先放入磁盘控制器缓冲区.
*	数据准备完成以后,磁盘向CPU发起I/O中断.
*	CPU收到I/O中断以后将磁盘缓冲区中的数据拷贝到内核缓冲区,然后再从内核缓冲区拷贝到用户缓冲区.
*	用户进程由内核态切换回用户态,解除阻塞状态,然后等待CPU的下一个执行时钟.

####	DMA传输原理

DMA的全称叫直接内存存取(Direct Memory Access),是一种允许外围设备(硬件子系统)直接访问系统主内存的机制.

也就是说,基于DMA访问方式,系统主内存与硬盘或网卡之间的数据传输可以绕开CPU的全程调度

目前大多数的硬件设备,包括磁盘控制器,网卡,显卡以及声卡等都支持DMA技术.
![K9B6QP.png](https://s2.ax1x.com/2019/10/15/K9B6QP.png)

整个数据传输操作在一个DMA控制器的控制下进行.CPU除了在数据传输开始和结束时做一点处理外(开始和结束时候要做中断处理),在传输过程中CPU可以继续进行其他的工作.这样在大部分时间里,CPU计算和I/O操作都处于并行操作,使整个计算机系统的效率大大提高.
![K9DvB8.png](https://s2.ax1x.com/2019/10/15/K9DvB8.png)

有了DMA磁盘控制器接管数据读写请求后,CPU从繁重的I/O操作中解脱,数据读取操作的流程如下:
*	用户进程向CPU发起read系统调用读取数据,由用户态切换为内核态,然后一直阻塞等待数据的返回.
*	CPU在接收到指令以后对DMA磁盘控制器发起调度指令.
*	DMA磁盘控制器对磁盘发起I/O请求,将磁盘数据先放入磁盘控制器缓冲区,CPU全程不参与此过程.
*	数据读取完成后,DMA磁盘控制器会接收到磁盘的通知,将数据从磁盘控制器缓冲区拷贝到内核缓冲区.
*	DMA磁盘控制器向CPU发出数据读完的信号,由CPU负责将数据从内核缓冲区拷贝到用户缓冲区.
*	用户进程内核态切换回用户态,解除阻塞状态,然后等待CPU的下一个执行时钟.


##	传统I/O方式

为了更好的理解零拷贝解决的问题,首先来了解一下传统I/O方式存在的问题.

在Linux系统中,传统的访问方式是通过write和read两个系统调用实现的,通过read函数读取文件到用户缓冲区,然后通过write方法把用户缓冲区中的数据输出到Socket缓冲区.下图分别对应传统I/O操作的数据读写流程,整个过程涉及2次CPU拷贝,2次DMA拷贝,总共4次拷贝,以及4次上下文切换.

![K9gZtS.png](https://s2.ax1x.com/2019/10/15/K9gZtS.png)

下面简单地阐述一下相关的概念:
*	上下文切换:当用户程序向内核发起系统调用时,CPU将用户进程从用户态切换到内核态;当系统调用返回时,CPU将用户进程从内核态切换回用户态.
*	CPU拷贝:由CPU直接处理数据的传送,数据拷贝时会一直占用CPU的资源
*	DMA拷贝:由CPU向DMA磁盘控制器下达指令,让DMA控制器来处理数据的传送,数据传送完毕再把信息反馈给CPU,从而减轻了CPU资源的占有率.

####	传统读操作

当应用程序执行read系统调用读取一块数据的时候,如果这块数据已经存在于用户进程的页缓存中,就直接从页缓存中读取数据;如果数据不存在,则先将数据从磁盘加载数据到内核空间的缓冲区中,再从读缓存拷贝到用户进程的缓冲区.

基于传统的I/O读取方式,read系统调用会触发2次上下文切换,1次DMA拷贝和1次CPU拷贝.

发起数据读取的流程如下:
*	用户进程通过read()函数向内核(kernel)发起系统调用,上下文从用户态(user space)切换为内核态(kernel space).
*	CPU利用DMA控制器将数据从主存或硬盘拷贝到内核空间(kernel space)的缓冲区.
*	CPU将缓冲区中的数据拷贝到用户空间(user space)的用户缓冲区.
*	上下文从内核态(kernel space)切换回用户态(user kernel),read调用执行返回.

####	传统写操作

当应用程序准备好数据,执行write系统调用发送网络数据时,先将数据从用户空间的页缓存拷贝到内核空间的缓冲区中,然后再将缓存中的数据拷贝到网卡设备完成数据发送.

基于传统的I/O写入方式,write系统调用会触发2次上下文切换,1次CPU拷贝和1次DMA拷贝.

用户程序发送网络数据的流程如下:
*	用户进程通过write函数向内核(kernel)发起系统调用,上下文从用户态(user space)切换为内核态(kernel space).
*	CPU将用户缓冲区中的数据拷贝到内核缓冲区.
*	CPU利用DMA控制器将数据从缓冲区拷贝到网卡进行数据传输.
*	上下文从内核态(kernel space)切换回用户态(user space),write系统调用执行返回.



##	零拷贝方式

在Linux中零拷贝技术主要有3个实现思路:
*	用户态直接I/O:应用程序可以直接访问硬件存储,操作系统内核只是辅助数据传输.<br>
这种方式依旧存在用户空间和内核空间的上下文切换,硬件上的数据直接拷贝至了用户空间,不经过内核空间.因此,直接I/O不存在内核空间缓冲区和用户空间缓冲区之间的数据拷贝.

*	减少数据拷贝次数:在数据传输过程中,避免数据在用户空间和系统内核空间缓冲区之间的CPU拷贝,以及数据在系统内核空间内的CPU拷贝,这也是当前主流零拷贝技术的实现思路.

*	写时复制技术:写时复制指的是当多个进程共享同一块数据时,如果其中一个进程需要对这份数据进行修改,那么将其拷贝到自己的进程地址空间中,如果只是数据读取操作则不需要进行拷贝操作.

####	用户态直接I/O

用户态直接I/O使得应用程序或运行在用户态(user space)下的库函数直接访问硬件设备.数据直接跨过内核进行传输,内核在数据传输过程除了进行必要的虚拟存储配置工作,不参与任何其他工作,这种方式能够直接绕过内核,极大提高了性能.
![KCZ5qO.png](https://s2.ax1x.com/2019/10/15/KCZ5qO.png)

用户态直接I/O只能适用于不需要内核缓冲区处理的应用程序,这些应用程序通常在进程地址空间有自己的数据缓存机制,称为自缓存应用程序,如数据库管理系统就是一个代表.

其次,这种零拷贝机制会直接操作磁盘I/O,由于CPU和磁盘I/O之间的执行时间差距,会造成大量的资源浪费,解决方案是配合异步I/O使用.


####	mmap + write

一种零拷贝方式是使用 mmap + write 代替原来的read write 方式,减少了1次CPU拷贝操作.mmap是Linux提供的一种内存映射文件方法,即将一个进程的地址空间中的一段虚拟地址映射到磁盘文件地址.

使用mmap的目的是将内核缓冲区的地址与用户空间缓冲区进行映射,从而实现内核缓冲区与应用程序内存的共享,省去了将数据从内核缓冲区拷贝到用户缓冲区的过程.

然而内核仍然需要将磁盘数据拷贝到内核缓冲区中.大致流程如下:
![KCwPDU.png](https://s2.ax1x.com/2019/10/15/KCwPDU.png)

基于 mmap + write 系统调用的零拷贝方式,整个拷贝过程会发生4次上下文切换,1次CPU拷贝和2次DMA拷贝.

用户读写数据的流程如下:
*	用户进程通过mmap向内核发起系统调用,上下文从用户态切换为内核态
*	将用户进程的内核空间的缓冲区与用户空间的缓冲区进行虚拟内存地址映射
*	CPU利用DMA控制器将数据从磁盘拷贝到内核空间的缓冲区
*	上下文从内核态切换回用户态,mmap系统调用执行返回
*	用户进程通过write函数向内核发起系统调用,上下文从用户态切换为内核态
*	CPU将内核缓冲区中的数据拷贝到网络缓冲区
*	CPU利用DMA控制器将数据从网络缓冲区拷贝到网卡进行数据传输
*	上下文从内核态切换回用户态,write系统调用执行返回.


mmap主要的用处是提高I/O性能,特别是针对大文件.对于小文件,内存映射文件反而会导致碎片空间的浪费.因为内存映射总是要对齐页边界,最小单位是4KB,一个5KB的文件将会映射占用8KB内存,也就会浪费3KB内存.

mmap的拷贝虽然减少了1次拷贝,提升了效率,但也存在一些隐藏的问题.

**当mmap一个文件时,如果这个文件被另一个进程所截获,那么write系统调用会因为访问非法地址被SIGBUS信号终止,SIGBUS默认会杀死进程并产生一个coredump,服务器可能因此被终止.**

####	Sendfile

Sendfile系统调用在Linux内核版本2.1中被引入,目的是简化通过网络在两个通道之间进行的数据传输过程.

Sendfile系统调用的引入,不仅减少了CPU拷贝的次数,还减少了上下文切换的次数.通过Sendfile系统调用,数据可以直接在内核空间内部进行I/O传输,从而省去了数据在用户空间和内核空间之间的来回拷贝.

与mmap内存映射方式不同的是,Sendfile调用中I/O数据对用户空间是完全不可见的.也就是说,这是一次完全意义上的数据传输过程.
![KCRh5V.png](https://s2.ax1x.com/2019/10/15/KCRh5V.png)

基于Sendfile系统调用的零拷贝方式,整个拷贝过程会发生2次上下文切换,1次CPU拷贝和2次DMA拷贝

用户程序读写数据的流程如下:
*	用户进程通过sendfile()函数向内核发起系统调用,上下文从用户态切换为内核态.
*	CPU利用DMA控制器将数据从硬盘拷贝到内核空间的缓冲区
*	CPU将内核缓冲区中的数据拷贝到网络缓冲区
*	CPU利用DMA控制器将数据从网络缓冲区拷贝到网卡进行数据传输.
*	上下文从内核态切换回用户态,Sendfile系统调用执行返回.

相比较于mmap没存映射的方式,Sendfile少了2次上下文切换,但是仍然有1次CPU拷贝操作.

**Sendfile存在的问题是用户程序不能对数据进行修改,而只是单纯地完成了一次数据传输过程.**


####	Sendfile+DMA gather copy

Linux 2.4版本的内核对Sendfile系统调用进行修改,为DMA拷贝引入了gather操作.

它将内核空间的缓冲区中对应的数据描述信息(内存地址,地址偏移量)记录到相应的网络缓冲区中,由DMA根据内存地址,地址偏移量将数据批量地从缓冲区拷贝到网卡设备中.

这就省去了内核空间中仅剩的1次CPU拷贝操作.在硬件的支持下,Sendfile拷贝方式不再从内核缓冲区的数据拷贝到socket缓冲区,取而代之的仅仅是缓冲区文件描述符和数据长度的拷贝.这样DMA引擎直接利用gather操作将页缓存中数据打包发送到网络中即可,本质就是和虚拟内存映射的思路类似.
[![KCIl4I.png](https://s2.ax1x.com/2019/10/15/KCIl4I.png)](https://imgchr.com/i/KCIl4I)

基于Sendfile+DMA gather copy系统调用的零拷贝方式,整个拷贝过程会发生2次上下文切换,0次CPU拷贝以及2次DMA拷贝.

用户程序读写数据的流程如下:
*	用户进程通过sendfile()函数向内核发起系统调用,上下文从用户态切换为内核态.
*	CPU利用DMA控制器将数据从磁盘拷贝到内核空间的缓冲区
*	CPU把内核缓冲区的文件描述符和数据长度拷贝到网络缓冲区
*	基于已拷贝的文件描述符和数据长度,CPU利用DMA控制器的gather/scatter操作直接批量地将数据从内核的缓冲区拷贝到网卡进行数据传输.
*	上下文从内核态切换回用户态,Sendffile系统调用执行返回.

**Sendfile + DMA gather copy拷贝方式同样存在用户程序不能对数据进行修改的问题,而且本身需要硬件的支持,它只适用于将数据从文件拷贝到socket套接字上的传输过程.**


####	Splice

**Sendfile只适用于将数据从文件拷贝到socket套接字上,同时需要硬件的支持,这也限定了它的适用范围.**<br>
Linux 在2.6.17版本引入Splice系统调用,不仅不需要硬件支持,还实现了两个文件描述符之间的数据零拷贝.

Splice系统调用可以在内核空间缓冲区和网络缓冲区之间建立管道(pipeline),从而避免了两者之间的CPU拷贝操作.
![KiZ2zn.png](https://s2.ax1x.com/2019/10/16/KiZ2zn.png)

基于Splice系统调用的零拷贝方式,整个拷贝过程会发生2次上下文切换,0次CPU拷贝以及2次DMA拷贝.

用户程序读写数据的流程如下:
*	用户进程通过splice()函数向内核发起系统调用,上下文从用户态切换为内核态.
*	CPU利用DMA控制器将数据从磁盘拷贝到内核缓冲区.
*	CPU在内核空间的缓冲区和网络缓冲区之间建立管道
*	CPU利用DMA控制器将数据从网络缓冲区拷贝到网卡进行数据传输
*	上下文从内核态切换回用户态,Splice系统调用执行返回.


**Splice拷贝方式也同样存在于用户程序不能对数据进行修改的问题.除此之外,它使用了Linux的管道缓冲机制,可以用于任意两个文件描述符中传输数据,但是它的两个文件描述符参数中有一个必须是管道设备.**


####	写时复制

在某些情况下,内核缓冲区可能被多个进程所共享,如果某个进程想要对这个共享区进行write操作,由于write不提供任何的锁操作,那么就会对共享区中的数据造成破坏,写时复制的引入就是Linux用来保护数据的

写时复制指的是当多个进程共享同一块数据时,如果其中一个进程需要对这份数据进行修改,那么就需要将其拷贝到自己的进程地址空间中.

这样做并不影响其他进程对这个数据的操作,每个进程要修改的时候才会进行拷贝,所以叫写时复制.

这种方法在某种程度上能够降低系统开销,如果某个进程永远不会对所访问的数据进行更改,那么也就永远不需要拷贝.


####	缓冲区共享

缓冲区共享方式完全改写了传统的I/O操作,因为传统I/O接口都是基于数据拷贝进行的,要避免拷贝就得去掉原先的那套接口并重新改写.所以这种方法是比较全面的零拷贝技术,目前比较成熟的一个方案是在Solaris上实现的fbuf(Fast Buffer,快速缓冲区).

fbuf的思想是每个进程都维护着一个缓冲区池,这个缓冲区池能被同时映射到用户空间和内核态,内核和用户共享这个缓冲区池,这样就避免了一系列的拷贝操作.
![KiKaHx.png](https://s2.ax1x.com/2019/10/16/KiKaHx.png)
缓冲区共享的难度在于管理共享缓冲区池需要应用程序,网络软件以及设备驱动程序之间的紧密合作,而且如何改写API目前还处于试验阶段并不成熟.



##	Linux零拷贝对比
无论是传统I/O拷贝方式还是引入零拷贝的方式,2次DMA Copy是都少不了的,因为两次DMA都是依赖硬件完成的.

下面从CPU拷贝次数,DMA拷贝次数以及系统调用几个方面总结一下上述几种I/O拷贝方式的差别:
![KiQMfU.png](https://s2.ax1x.com/2019/10/16/KiQMfU.png)


##	零拷贝的广义狭义之分

广义零拷贝:	能减少拷贝次数,减少不必要的数据拷贝,就算作"零拷贝".这是目前,对零拷贝最为广泛的定义,这是广义上的零拷贝,并不是操作系统意义上零拷贝.
狭义零拷贝:	操作系统意义上的零拷贝.


##	Kafka中的零拷贝

Kafka两个重要过程都使用了零拷贝技术,且都是操作系统层面的狭义零拷贝,一是Producer生产的数据存到broker,二是Consumer从broker读取数据.
*	Producer生产的数据持久化到broker,采用mmap文件映射,实现顺序的快速写入
*	Customer从broker读取数据,采用sendfile,将磁盘文件读到OS内核缓冲区后,直接转到socket buffer进行网络发送.(sendfile)


##	RocketMQ和Kafka对比

RocketMQ选择了mmap + write 这种零拷贝方式,适用于业务级消息这种小块文件的数据持久化和传输.

而Kafka采用的是Sendfile这种零拷贝方式,适用于系统日志消息这种高吞吐量的大块文件的数据持久化和传输.

但是值得注意的一点是,Kafka的索引文件使用的是mmap + write方式,数据文件使用的是Sendfile方式.

![Kil8C8.png](https://s2.ax1x.com/2019/10/16/Kil8C8.png)



