---
layout:     post
title:      "kafka consumer"
date:       2021-05-31 14:00:00 +0800
author:     "simba"
header-img: "img/post-bg-miui6.jpg"
tags:
    - MQ

---







## 导航
[一. 消费者与消费者组](#jump1)
<br>
[二. 消费流程](#jump2)
<br>
[三. 位移提交](#jump3)
<br>
[四. rebalance](#jump4)
<br>
[五. 重要的消费者参数](#jump5)
<br>









<br><br>
## <span id="jump1">一. 消费者与消费者组</span>

消费者( Consumer)负责订阅 Kafka 中的主题( Topic)，并且从订阅的主题上拉取消息。在 Kafka 的消费理念中还有一层消费组( Consumer Group) 的概念，每个消费者都有 一个对应的消费组。当消息发布到主题后，只会被投递给订阅它的每 个消费组中的一个消费者 。<br>

如下图所示,某个主题中共有 4 个分区( Partition) : PO、 Pl、 P2、 P3。 有两个消费组 A 和 B 都订阅了这个主题，消费组 A 中有 4 个消费者 (CO、 Cl、 C2 和 C3)，消费组 B 中有 2 个消费者 CC4 和 CS) 。按照 Kafka默认的规则，最后的分配结果是消费组 A 中的每一个消费 者分配到 l 个分区，消费组 B 中的每一个消费者分配到 2个分区，两个消费组之间互不影响。 每个消费者只能消费所分配到的分区中的消息。换言之 ，每一个分区只能被一个消费组中的一 个消费者所消费 。<br>

[![2elU0A.png](https://z3.ax1x.com/2021/05/31/2elU0A.png)](https://imgtu.com/i/2elU0A)



<br><br>
## <span id="jump2">二. 消费流程</span>

1. 配置消费者客户端参数及创建相应的消费者实例
2. 订阅主题
3. 拉取消息并消费
4. 提交消费位移
5. 关闭消费者实例


消息消费模式: kafka中的消费是基于poll模式的.对于 poll()方法而言，如果某些分区中没有可供消费的消息，那么此分区对应的消息拉取的 结果就为空; 如果订阅的所有分区中都没有可供消费的消息，那么 poll()方法返回为空的消息集合.poll()方法里还有一个超时时间参数 timeout，用来控制 poll()方法的阻塞时间，在 消费者的缓冲区里没有可用数据时会发生阻塞 。<br>

timeout 的设置取决于应用程序对响应速度的要求，比如需要在多长时间内将控制权移交 给执行轮询的应用线程。可以直接将 timeout 设置为 0， 这样 poll()方法会立刻返回，而不管 是否己经拉取到了消息。如果应用线程唯一的工作就是从 Kafka 中拉取并消费消息，则可以将 这个参数设置为最大值 Long.MAX_VALUE。



<br><br>
## <span id="jump3">三. 位移提交</span>

kafka的消费位移存储在Kafka内部的主题 \_\_consumer\_offsets 中 。这里把将消费位移存储起来(持久化)的动作称为"提交"，消费者在消费完消息之后需要执行消费位移的提交。<br>

位移提交的方式:
* 自动提交
    > 在 Kafka 中默认的消费位移的提交方式是自动提交，这个由消费者客户端参数 enable.auto.commit 配置，默认值为 true。当然这个默认的自动提交不是每消费一条消息就提交一次，而是定期提交，这个定期的周期时间由客户端参数 auto.commit.interval.ms 配置，默认值为 5 秒，此参数生效的前提是 enable.auto.commit 参数为 true。在默认的方式下，消费者每隔 5秒会将拉取到的每个分区中最大的消息位移进行提交。 自动位移提交的动作是在 poll()方法的逻辑里完成的，在每次真正向服务端发起拉取请求之前会检 查是否可以进行位移提交，如果可以，那么就会提交上一次轮询的位移。
* 手动提交 ---- 开启手动提交功能的前提是消费者客户端参数 enable.auto.commit 配置为 false.手动提交可分为同步和异步
    * 同步 ---- 手动调用commitSync()方法
        > commitSync()方法会根据 poll()方法拉取的最新位移来进行提交,只要没有发生不可恢复的错误,它就会阻塞消费者 线程直至位移提交完成
    * 异步 ---- 手动调用commitAsync()方法
        > commitAsync()在执行的时候消费者线程不会被阻塞，可能在提交消费位移的结果还未返回之前就开始了新一次的拉取操作。异步提交可 以便消费者的性能得到一定的增强。但同时也引入了异步提交失败导致的异常重复消费问题.

<font color="red">这里有一点需要注意,不论是自动提交还是手动提交(包括同步和异步),这里的目标场景都是:当consumer宕机或者有新consumer消费时,可以获取到该组的消费进度.但是!!!!!在consumer正常运行过程当中,每次poll消息并不是broker去现场查询提交的offset是多少再去返回消息.而是consumer内部会缓存消费位移,每次poll内部会更新下次要poll的位移,以该位移来请求broker获取消息.这点稍微想想就会明白,如果真是每次broker现场查询消费进度的话,那么在自动提交场景下,默认5秒内,那岂不是每次都返回同一批消息了吗!!!!!</font> <br>



<br><br>
## <span id="jump4">四. rebalance</span>

在再均衡发生期间，消费组内的消费者是无法读取消息的。也就是说，在再均衡发生期间的这一小段时间内，消费组会变得不可用。另外，当一个分区被重新分配给另一个消费者时，消费者当前的状态也会丢失。比如消费者消费完某个分区中的一部分消息时还没有来得及提交消费位移就发生了再均衡操作，之后这个分区又被分配给了消费 内的另一个消费者，原来被消费完的那部分消息又被重新消费一遍，也就是发生了重复消费。一般情况下，应尽量避免不必要的再均衡的发生。<br>



<br><br>
## <span id="jump5">五. 重要的消费者参数</span>

<br>
**<font size="5">fetch.min.bytes</font>** <br>

该参数用来配置 Consumer 在一次拉取请求(调用 poll()方法)中能从 Kafka 中拉取的最小数据量，默认值为 1 (B) 。 Kafka在收到 Consumer 的拉取请求时，如果返回给 Consumer 的数据量小于这个参数所配置的值，那么它就需要进行等待，直到数据量满足这个参数的配置大小。 可以适当调大这个参数的值以提高一定的吞吐量，不过也会造成额外的延迟(latency) ，对于延迟敏感的应用可能就不可取了。<br>


<br>
**<font size="5">fetch.max.bytes</font>** <br>

该参数与 fetch.max.bytes 参数对应，它用来配置 Consumer在一次拉取请求中从 Kafka 中拉取的最大数据量，默认值为 52428800 (B)，也就是 50MB。如果这个参数设置的值比任何一条写入 Kafka 中的消息要小，那么会不会造成无法消费呢? 很多资料对此参数的解读认为 是无法消费的，比如一条消息的大小为 1OB，而这个参数的值是 1 (B)，既然此参数设定的值是一次拉取请求中所能拉取的最大数据量，那么显然 1B \< 1OB， 所以无法拉取。这个观点是错误的，该参数设定的不是绝对的最大值 ，如果在第一个非空分区中拉取的第一条消息大于该值， 那么该消息将仍然返回，以确保消费者继续工作。 也就是说，上面问题的答案是可以正常消费。 与此相关的，Kafka中所能接收的最大消息的大小通过服务端参数 message.max.bytes (对应于主题端参数 max.message.bytes)来设置。<br>


<br>
**<font size="5">fetch.max.wait.ms</font>** <br>

这个参数也和 fetch.min.bytes 参数有关，如果 Kafka 仅仅参考 fetch.min.bytes 参数的要求，那么有可能会一直阻塞等待而无法发送响应给 Consumer， 显然这是不合理的。fetch.max.wait.ms 参数用于指定 Kafka 的等待时间，默认值为 500 (ms)。如果 Kafka 中没有足够多的消息而满足不了 fetch.min.bytes 参数的要求，那么最终会等待 500ms。 这个参数的设定和 Consumer 与 Kafka 之间的延迟也有关系，如果业务应用对延迟敏感，那么可以适 当调小这个参数。<br>


<br>
**<font size="5">max.partition.fetch.bytes</font>** <br>

这个参数用来配置从每个分区里返回给 Consumer的最大数据量 ，默认值为 1048576 (B) ,即 1MB。这个参数与 fetch.max.bytes 参数相似，只不过前者用来限制一次拉取中每个分区的消息大小，而后者用来限制一次拉取中整体消息的大小。同样，如果这个参数设定的值比 消息的大小要小，那么也不会造成无法消费，Kafka 为了保持消费逻辑的正常运转不会对此做强硬的限制。<br>


<br>
**<font size="5">max.poll.records</font>** <br>

这个参数用来配置 Consumer 在一次拉取请求中拉取的最大消息数，默认值为 500 (条)。如果消息的大小都比较小，则可以适当调大这个参数值来提升一定的消费速度。<br>


<br>
**<font size="5">retry.backoff.ms</font>** <br>

这个参数用来配置尝试重新发送失败的请求到指定的主题分区之前的等待(退避)时间，避免在某些故障情况下频繁地重复发送，默认值为 100 (ms)。<br>


