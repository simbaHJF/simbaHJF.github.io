---
layout:     post
title:      "分布式系统--Paxos"
date:       2021-04-19 15:00:00 +0800
author:     "simba"
header-img: "img/post-bg-miui6.jpg"
tags:
    - 分布式

---








## 导航
[一. 经典Paxos算法要解决的问题](#jump1)
<br>
[二. Paxos算法的角色划分](#jump2)
<br>
[三. Proposer生成提案与Acceptor批准提案:Paxos算法描述](#jump3)
<br>
[四. 通过选取主Proposer保证算法的活性](#jump4)
<br>
[五. 一点总结](#jump5)
<br>
[六. ZAB和Raft的一点对比](#jump6)
<br>









<br><br>
## <span id="jump1">一. 经典Paxos算法要解决的问题</span>

Paxos算法需要解决的问题是,如何在一个可能发生宕机或网络异常的分布式系统中,快速且正确地在集群内部对某个数据的值达成一致,并且保证不论发生以上任何异常,都不会破坏整个系统的一致性.<br>

<font color="red">这里某个数据的值并不只是狭义上的某个数,它可以是一条日志,也可以是一条命令,根据应用场景不同,某个数据的值有不同的含义.</font>



<br><br>
## <span id="jump2">二. Paxos算法的角色划分</span>

* Proposer:	提出提案  
* Acceptor:	接收提案  
* Learner:	复制Acceptor的状态(即Acceptor告诉Learner哪个Value被选定了)

在Paxos算法描述场景中,会有多个Proposer提出议案(也即数据),<font color="red">每个节点在协议中可以担任多个角色</font>.如下图所示
[![coRZ0f.png](https://z3.ax1x.com/2021/04/19/coRZ0f.png)](https://imgtu.com/i/coRZ0f)

图中5个server组成了Paxos算法集群,每个client都可以向server集群中的任意节点发送数据.这点是与Raft不同的,Raft中,只有leader节点可以接收写命令.<br>

在这个过程中,比如server1接收到了client的数据提交,那么此时server1将作为Proposer进行提出提案的操作,这时其他的server就作为Acceptor(这里包括server1,因此server1节点此时既作为Proposer提出提案,又作为Acceptor接受提案).<br>



<br><br>
## <span id="jump3">三. Proposer生成提案与Acceptor批准提案:Paxos算法描述</span>

<br>
**<font size="4">Proposer生成提案,也即阶段一</font>** <br>

1.	Proposer选择一个提案编号Mn,然后向Acceptor集群成员发送编号为Mn的Prepare请求.
2.	如果一个Acceptor收到一个编号为Mn的Prepare请求,且编号Mn大于该Acceptor已经响应的所有Prepare请求的编号,那么它就会将它已经批准过的最大编号的提案作为响应反馈给Proposer,同时该Acceptor会承诺不会再批准任何编号小于Mn的提案

在确定提案之后,Proposer就会将该提案在此发送给某个Acceptor集合,并期望获得它们的批准,我们称此请求为Accept请求.需要注意的一点是,此时接受Accept请求的Acceptor集合不一定是之前响应Prepare请求的Acceptor集合----这点相信读者也能够明白,任意两个半数以上的Acceptor集合,必定包含至少一个公共Acceptor.<br>


<br>
**<font size="4">Acceptor批准提案,也即阶段二</font>** <br>

1.	如果Proposer收到来自半数以上的Acceptor对于其发出的编号为Mn的Prepare请求的响应,那么它就会发送一个针对[Mn,Vn]提案的Accept请求给Acceptor.注意,Vn的值就是收到的响应中编号最大的提案的值,如果响应中不包含任何提案,那么它就是任意值.
2.	如果Acceptor收到这个针对[Mn,Vn]提案的Accept请求,只要该Acceptor尚未对编号大于Mn的Prepare请求做出响应,它就可以通过(批准)这个提案.



<br><br>
## <span id="jump4">四. 通过选取主Proposer保证算法的活性</span>

根据前面的内容讲解,我们已经基本上了解了Paxos算法的核心逻辑,下面我们再来看看Paxos算法在实际作过程中的一些细节.假设存在这样一种极端情况,有两个Proposer依次提出了一系列编号递增的议案,但是最终都无法被选定,具体流程如下:<br>

Proposer P1提出了一个编号为M1的提案,并完成了上述阶段一的流程.但与此同时,另外一个Proposer P2提出了一个编号为M2(M2>M1)的提案,同样也完成了阶段一的流程,于是Acceptor已经承诺不再批准编号小于M2的提案了.因此,当P1进入阶段二的时候,其发出的Accept请求将被Acceptor忽略,于是P1再次进入阶段一并提出了一个编号为M3(M3>M2)的提案,而这又导致P2在第二阶段的Accept请求被忽略,以此类推,提案的选定过程将陷入死循环.<br>

<font color="red">因此,原始经典base-paxos这种方案达成一件事情的一致性还好,面对多件事情的一致性就比较复杂了,所以通过选举出一个leader来简化实现的复杂性比较符合需求.</font> <br>

为了保证Paxos算法流程的可持续性,以避免陷入上述提到的"死循环",就必须选择一个主Proposer,并规定只有主Proposer才能提出议案.这样一来,只要主Proposer和过半的Acceptor能够正常进行网络通信,那么但凡主Proposer提出一个编号更高的提案,该提案终将会被批准.因此,如果系统中有足够多的组件(包括Proposer,Acceptor和其他网络通信组件)能够正常工作,那么通过选择一个主Proposer,整套Paxos算法流程就能够保持活性.<br>



<br><br>
## <span id="jump5">五. 一点总结</span>

Paxos的描述场景是,存在多个Proposer可以提出提案(可理解为发起分布式数据写入),因此,Paxos算法分为两个阶段,首先进行提案编号询问(也即阶段一,Proposer生成提案,发出prepare请求),阶段一成功后再进行阶段二(发出accept请求,Acceptor批准提案),但这里阶段二就会存在上面第四节描述的问题,因此阶段二不能确保一定成功,哪怕在网络良好未出现分区的情况下.<br>

而为了避免前面的这个问题,就提出了选主的思想(leader才能执行写入请求,也即Proposer变成了一个),此时就不会出现这个问题了,这时是不是就很像Raft了呢?对吧.<br>

Raft简化了场景,规定只允许存在一个节点能操作写入,即leader.对于leader可能的宕机而造成的单点问题,采用故障恢复的选主来进行故障转移,完美解决,同时实现起来逻辑清晰,界限分明,不像Paxos那么模糊复杂.<br>



<br><br>
## <span id="jump6">六. ZAB和Raft的一点对比</span>

ZAB角色划分:
* Leader: 能处理写请求
* Follower: 接受Leader的数据同步,参与投票
* Observer: 复制备份,不能参与投票


leader选举的效率:
* Raft中的每个server在某个term轮次内只能投一次票,哪个candidate先请求投票谁就可能先获得投票,这样就可能造成split vote,即各个candidate都没有收到过半的投票,Raft通过candidate设置不同的超时时间,来快速解决这个问题,使得先超时的candidate(在其他人还未超时时)优先请求来获得过半投票
* ZooKeeper中的每个server,在某个electionEpoch轮次内,可以投多次票,只要遇到更大的票就更新,然后分发新的投票给所有人.这种情况下不存在split vote现象,同时有利于选出含有更新更多的日志的server,但是选举时间理论上相对Raft要花费的多,也就是说ZK的选举中,一个electionEpoch一定可以选出一个leader,然后在这个electionEpoch下leader一直保持服务生命周期,除非这个leader宕机了,那就开启下一个electionEpoch,重新从选举开始

上一轮次的leader的残留的数据的处理问题:
* Raft:Raft对于之前term的entry被过半还是未过半复制的日志采取的是保守的策略,即暂不提交,只有当本term的数据提交了才能将之前term的数据一起提交
* ZooKeeper: ZooKeeper每次leader选举之后都会有一个数据同步阶段,采取激进的策略,对于所有过半还是未过半的日志都判定为提交,都将其应用到状态机中

Raft的保守策略更多是因为Raft在leader选举完成之后,没有同步更新过程来保持和leader一致(在可以对外处理请求之前的这一同步过程).而ZooKeeper是有该过程的.所以ZAB从选主到能够再次对外服务是比较比较慢的,因为多了一个过程.<br>




ZooKeeper和Raft在一旦分区发生的情况下是是牺牲了高可用来保证一致性,即CAP理论中的CP.但是在没有分区发生的情况下既能保证高可用又能保证一致性,所以更想说的是所谓的CAP二者取其一,并不是说该系统一直保持CA或者CP或者AP,而是一个会变化的过程.在没有分区出现的情况下,既可以保证C又可以保证A,在分区出现的情况下,那就需要从C和A中选择一样.ZooKeeper和Raft则都是选择了C.